define flow main
flow
  use integration;
  use std::time::nanos;
  use troy::connectors;

  define connector in from metronome
  with
    metrics_interval_s = 3,
    config = { "interval": nanos::from_seconds(1) }
  end;
  create connector in;

  define connector out from file
  with
    metrics_interval_s = 3,
    codec = "json-sorted",
    postprocessors = ["lines"],
    config = {
      "path": "events.log",
      "mode": "write"
    }
  end;
  create connector out;

  create connector metrics_pipeline from integration::write_file
  with
    file = "metrics_pipeline.log"
  end;
  create connector metrics_source from integration::write_file
  with
    file = "metrics_source.log"
  end;
  create connector metrics_sink from integration::write_file
  with
    file = "metrics_sink.log"
  end;
  create connector exit from connectors::exit;
  create connector metrics from connectors::metrics;
  create connector stdio from connectors::console;

  define pipeline main
  args
    metrics_interval_s = 3
  pipeline
    #!config metrics_interval_s = 3

    define script process
    script
      let event.ingest_ns = null;
      event
    end;

    create script process;

    select event from in into process;
    select event from process/out into out;
    select event from process/err into err;

  end;
  create pipeline main;
  define pipeline metrics
  into out, err, `pipeline`, source, sink
  pipeline
    define script process
    script
      let event.timestamp = null; # for testing

      let state = match state of
        # exit when we have all metrics events
        case %{ present source, present `pipeline`, present sink } when state.source >= 9 and state.`pipeline` >= 4 and state.sink >= 6 => emit {"exit": 0, "delay": 0} => "done"
        # state is valid
        case %{} => state
        # initialize state
        default => {
          "out": 0,
          "source": 0,
          "pipeline": 0,
          "sink": 0
        }
      end;

      let port = match event of
        case %{measurement == "events"} => "pipeline"
        # {"fields":{"count":1},"measurement":"connector_events","tags":{"connector":"in","port":"out"},"timestamp":null}
        case %{tags ~= %{`connector` == "out"}} => "source"
        # {"measurement":"connector_events","tags":{"connector":"in","port":"err"},"fields":{"count":0},"timestamp":1642415106128300784}
        case %{tags ~= %{`connector` == "in"}} => "sink"
        default => "out"
      end;
      # count events per port
      let state[port] = state[port] + 1;

      emit event => "#{port}";
    end;

    create script process;

    select event from in into process;
    select event from in into out;

    select event from process/`pipeline` into out/`pipeline`;
    select event from process/source into out/source;
    select event from process/sink into out/sink;
    select event from process/done into out/done;

    select event from process/err into err;

  end;
  create pipeline metrics;

  connect /connector/in to  /pipeline/main;
  connect /pipeline/main/out to /connector/out;

  connect /connector/metrics to /pipeline/metrics;
  connect /pipeline/metrics/`pipeline` to /connector/metrics_pipeline;
  connect /pipeline/metrics/source to /connector/metrics_source;
  connect /pipeline/metrics/sink to /connector/metrics_sink;
  # catching other outputs, just in case
  connect /pipeline/metrics/out to /connector/stdio/stdout;
  connect /pipeline/metrics/err to /connector/stdio/stderr;

  # exit when we have all events
  connect /pipeline/metrics/done to /connector/exit;

end;

deploy flow main;
